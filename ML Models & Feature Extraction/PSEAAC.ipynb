{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "je73ozUclH00"
   },
   "source": [
    "# Package Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DllUkhKAusPV"
   },
   "source": [
    "# Importing Relevant Libraries\n",
    "We will be using propy's pseudoAAC for feature extraction, numpy for data processing, matplotlib for graphs (AUC-ROC), tqdm for progress bars, imblearn for handling class imbalances and sklearn for classifiers and metrics,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8-SGd_euTVO"
   },
   "outputs": [],
   "source": [
    "from propy.PseudoAAC import GetAPseudoAAC\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    classification_report,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz-dAppJu0eY"
   },
   "source": [
    "# Parsing Data\n",
    "First, we need to read the data present in the .fasta files. This extracts the sequences and their appropriate labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVseoi8uupYJ"
   },
   "outputs": [],
   "source": [
    "def read_fasta(path):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    with open(path, \"r\") as f:\n",
    "        seq = \"\"\n",
    "        label = None\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "              if seq and label is not None:\n",
    "                  seq = seq.replace(\"X\", \"G\")\n",
    "                  seq = seq.replace(\"U\", \"G\")\n",
    "                  sequences.append(seq)\n",
    "                  labels.append(label)\n",
    "                  seq = \"\"\n",
    "              parts = line.split(\"_\")\n",
    "              label = int(parts[-1])\n",
    "            else:\n",
    "                seq += line\n",
    "        if seq and label is not None:\n",
    "            seq = seq.replace(\"X\", \"G\")\n",
    "            seq = seq.replace(\"U\", \"G\")\n",
    "            sequences.append(seq)\n",
    "            labels.append(label)\n",
    "    return sequences, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nq05ewRJYC4x"
   },
   "source": [
    "# Filtering by length\n",
    "Next, we filter the sequences for length. While this is not strictly necessary for the final phase, we used this function to test the method for smaller sequence lengths due to the heavy computational demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ta3iDokvu2wm"
   },
   "outputs": [],
   "source": [
    "def filter_length(sequences, labels, min_length=None, max_length=None):\n",
    "    sequences_filtered = []\n",
    "    labels_filtered = []\n",
    "    for seq, label in zip(sequences, labels):\n",
    "        seq_length = len(seq)\n",
    "        if min_length is not None and seq_length < min_length:\n",
    "            continue\n",
    "        if max_length is not None and seq_length > max_length:\n",
    "            continue\n",
    "        sequences_filtered.append(seq)\n",
    "        labels_filtered.append(label)\n",
    "    return sequences_filtered, labels_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dKlgQ30MYaT1"
   },
   "source": [
    "# Feature Extraction\n",
    "Here, we perform our feature extraction using propy's GetPseudoAAC function. The lamda parameter is the additional number of pseudoAAC descriptors/ features, adding on from the usual 20. It is a non-negative integer less than the length of the input protein sequence. The weight parameter allows for us to put additional weight onto these additional descriptors/ features, and this ranges from between 0.05 to 0.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4nTi-pfYdnI"
   },
   "outputs": [],
   "source": [
    "def compute_pseudoaac_features(sequences, lamda=10, weight=0.05):\n",
    "    features = []\n",
    "    for seq in tqdm(sequences, desc=\"Computing PseudoAAC features\"):\n",
    "        try:\n",
    "            feature_dict = GetAPseudoAAC(seq, lamda=lamda, weight=weight)\n",
    "            feature_values = [feature_dict[key] for key in sorted(feature_dict.keys())]\n",
    "            features.append(feature_values)\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing features for sequence: {seq[:30]}... {e}\")\n",
    "            features.append(None)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering\n",
    "We only select sequences without empty features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sequences_labels_features(sequences, labels, features):\n",
    "    sequences_filtered = []\n",
    "    labels_filtered = []\n",
    "    features_filtered = []\n",
    "    for seq, label, feat in zip(sequences, labels, features):\n",
    "        if feat is not None:\n",
    "            sequences_filtered.append(seq)\n",
    "            labels_filtered.append(label)\n",
    "            features_filtered.append(feat)\n",
    "    return sequences_filtered, labels_filtered, features_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-wFtsU9Y0zU"
   },
   "source": [
    "# Min/max length\n",
    "A minimum and maximum length is specified here, which we used to test on smaller samples of data. In our final run, we just set these bounds to include all possible sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25i3TARzY3FF"
   },
   "outputs": [],
   "source": [
    "MIN_SEQUENCE_LENGTH = 0\n",
    "MAX_SEQUENCE_LENGTH = float('inf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv_lC7K-ZBTo"
   },
   "source": [
    "# Reading and processing training data\n",
    "We read and process the training data and convert this information to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_data_path = os.path.join('..', 'Data', 'Train.fasta')\n",
    "test_data_path = os.path.join('..', 'Data', 'Test.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "n8dFLJEFZPeb",
    "outputId": "1b8a8519-02f3-4e55-e1f9-921aae391a9d"
   },
   "outputs": [],
   "source": [
    "# Read and process the training data\n",
    "train_sequences, train_labels = read_fasta(train_data_path)\n",
    "\n",
    "# Filter sequences based on length\n",
    "train_sequences_filtered, train_labels_filtered = filter_length(\n",
    "    train_sequences, train_labels, min_length=MIN_SEQUENCE_LENGTH, max_length=MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "# Compute PseudoAAC features for training data with progress bar\n",
    "train_features = compute_pseudoaac_features(train_sequences_filtered, lamda=10, weight=0.05)\n",
    "\n",
    "# Filter out sequences with invalid features\n",
    "train_sequences_final, train_labels_final, train_features_final = filter_sequences_labels_features(\n",
    "    train_sequences_filtered, train_labels_filtered, train_features\n",
    ")\n",
    "\n",
    "# Convert features and labels to NumPy arrays\n",
    "X_train = np.array(train_features_final)\n",
    "y_train = np.array(train_labels_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFQLHGo-aKDs"
   },
   "source": [
    "# Reading and processing testing data\n",
    "We read and process the testing data and convert this information to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUpXd_fbaLs6"
   },
   "outputs": [],
   "source": [
    "# Read and process the test data\n",
    "test_sequences, test_labels = read_fasta(test_data_path)\n",
    "\n",
    "# Filter sequences based on length\n",
    "test_sequences_filtered, test_labels_filtered = filter_length(\n",
    "    test_sequences, test_labels, min_length=MIN_SEQUENCE_LENGTH, max_length=MAX_SEQUENCE_LENGTH\n",
    ")\n",
    "\n",
    "# Compute PseudoAAC features for test data with progress bar\n",
    "test_features = compute_pseudoaac_features(test_sequences_filtered, lamda=10, weight=0.05)\n",
    "\n",
    "# Filter out sequences with invalid features\n",
    "test_sequences_final, test_labels_final, test_features_final = filter_sequences_labels_features(\n",
    "    test_sequences_filtered, test_labels_filtered, test_features\n",
    ")\n",
    "\n",
    "# Convert features and labels to NumPy arrays\n",
    "X_test = np.array(test_features_final)\n",
    "y_test = np.array(test_labels_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uc9JDyw_aNt-"
   },
   "source": [
    "# SMOTE for class imbalance (only on training)\n",
    "As we have identified a class imbalance, we attempt to correct it using SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YA67pI1WaSOQ"
   },
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98PNkU2bwB5c"
   },
   "source": [
    "# Feature Scaling\n",
    "As some of the classification models require feature scaled data, we do so here. This is important for linear regression and support vector machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbZWMLKuwEmj"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_res_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGttuUe-aV6y"
   },
   "source": [
    "# Classifiers\n",
    "We define a dictionary of classifiers with appropriate parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnhj-oR3aW6A"
   },
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"Logistic Regression\": LogisticRegression(class_weight=\"balanced\", max_iter=1000, random_state=42),\n",
    "        \"Naive Bayes\": GaussianNB(),\n",
    "        \"Support Vector Machine\": SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\", random_state=42),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(class_weight=\"balanced\", random_state=42),\n",
    "        \"K Nearest Neighbors\": KNeighborsClassifier(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ke_bUhE0wL3x"
   },
   "source": [
    "# Training classifiers, predicting and evaluating\n",
    "Here, we use decide which classifiers we want to use scaled data on and which classifiers we want to use the original data on. For each of these classifiers, we calculate the relevant evaluation metrics and also save the trained models as .joblib files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m0wnRlabwNuL"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "scaled_classifiers = [\"Logistic Regression\", \"Support Vector Machine\", \"K Nearest Neighbors\"]\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Starting training for {name}\")\n",
    "    try:\n",
    "        if name in scaled_classifiers:\n",
    "            # Use scaled data for these classifiers\n",
    "            clf.fit(X_train_res_scaled, y_train_res)\n",
    "            y_pred = clf.predict(X_test_scaled)\n",
    "            y_pred_prob = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "        else:\n",
    "            # Use original data for Random Forest, Naive Bayes, and Decision Tree\n",
    "            clf.fit(X_train_res, y_train_res)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            y_pred_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        print(f\"Finished training for {name}\")\n",
    "\n",
    "        # Save the trained model\n",
    "        # model_filename = f\"{name.replace(' ', '_')}_model.joblib\"\n",
    "        # joblib.dump(clf, model_filename)\n",
    "        # print(f\"Model saved as {model_filename}\")\n",
    "\n",
    "        # Evaluate the model\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "        conf_mat = confusion_matrix(y_test, y_pred)\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        print(\"ROC AUC Score:\", roc_auc)\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(conf_mat)\n",
    "        print(\"Matthews Correlation Coefficient:\", mcc)\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "\n",
    "        # Plot ROC curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.title(f'ROC Curve - {name}')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with {name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
