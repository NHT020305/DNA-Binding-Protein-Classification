{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17527,
     "status": "ok",
     "timestamp": 1730094178684,
     "user": {
      "displayName": "An Nguyen",
      "userId": "10376814865507008633"
     },
     "user_tz": -480
    },
    "id": "u0JXNep4FvgM"
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio import AlignIO\n",
    "from Bio.motifs import Motif\n",
    "from Bio.Align import AlignInfo\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import os\n",
    "from Bio import motifs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, accuracy_score, confusion_matrix, matthews_corrcoef,\n",
    "    classification_report\n",
    ")\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = os.path.join('..', 'Data', 'Train_valid.fasta')\n",
    "test_data_path = os.path.join('..', 'Data', 'test_cleaned.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4057,
     "status": "ok",
     "timestamp": 1730094214504,
     "user": {
      "displayName": "An Nguyen",
      "userId": "10376814865507008633"
     },
     "user_tz": -480
    },
    "id": "UdrYQt4FGNu2"
   },
   "outputs": [],
   "source": [
    "y_train = [int(record.description.split('_')[-1]) for record in SeqIO.parse(train_data_path,'fasta')]\n",
    "y_test = [int(record.description.split('_')[-1]) for record in SeqIO.parse(test_data_path,'fasta')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1730094215061,
     "user": {
      "displayName": "An Nguyen",
      "userId": "10376814865507008633"
     },
     "user_tz": -480
    },
    "id": "d_2kTtTuGkz7"
   },
   "outputs": [],
   "source": [
    "X_train = [str(record.seq) for record in SeqIO.parse(train_data_path,'fasta')]\n",
    "X_test = [str(record.seq) for record in SeqIO.parse(test_data_path,'fasta')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEcPsTnEGrMa"
   },
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRqi7kPQGsuN"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer for character-level features\n",
    "count_vectorizer = CountVectorizer(analyzer='char')\n",
    "\n",
    "# Fit and transform the train sequences\n",
    "X_train_count = count_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_count = count_vectorizer.fit_transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pu7ZfX8QG4cE"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nL3_OTedG6F4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Initialize TfidfVectorizer for character-level features\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='char')\n",
    "\n",
    "# Fit and transform the train sequences\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_tfidf = tfidf_vectorizer.fit_transform(X_test).toarray()\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_tfidf, y_train = smote.fit_resample(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0w6qNrFHWt4"
   },
   "source": [
    "# Apply on a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBB43yYzHVm3"
   },
   "outputs": [],
   "source": [
    "# Function to evaluate model and print metrics\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else model.decision_function(X_test)\n",
    "\n",
    "    # Confusion matrix and derived metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    print(f\"Sensitivity: {sensitivity}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"MCC: {mcc}\")\n",
    "    print(f\"AUC-ROC: {auc_roc}\")\n",
    "\n",
    "    # Manually plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'AUC-ROC = {auc_roc:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guess\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    return {\"sensitivity\": sensitivity, \"specificity\": specificity, \"accuracy\": accuracy, \"mcc\": mcc, \"auc_roc\": auc_roc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtQVcSIfHYo8"
   },
   "outputs": [],
   "source": [
    "# List of models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"K Neighbors Classifier\": KNeighborsClassifier(n_neighbors = 15),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize(data):\n",
    "    data = pd.DataFrame(data)\n",
    "    scaler = StandardScaler()\n",
    "    standardized_data = scaler.fit_transform(data)\n",
    "    standardized_df = pd.DataFrame(standardized_data, columns=data.columns)\n",
    "    return standardized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8SggJbYHeRH"
   },
   "source": [
    "## Using TF-IDF as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_scaled = standardize(X_train_tfidf)\n",
    "test_x_scaled = standardize(X_test_tfidf)\n",
    "\n",
    "# mcc_list = []\n",
    "\n",
    "# for k in range(1, 267, 2):\n",
    "#     knn = KNeighborsClassifier(n_neighbors=k)\n",
    "#     knn_pred = knn.fit(train_x_scaled, y_train)\n",
    "#     knn_pred = knn.predict(test_x_scaled)\n",
    "#     mcc_list.append([k, matthews_corrcoef(y_test, knn_pred)])\n",
    "\n",
    "# print(max(mcc_list, key=lambda x: x[1]))\n",
    "\n",
    "k = 37\n",
    "model = KNeighborsClassifier(n_neighbors=k)\n",
    "model.fit(train_x_scaled, y_train)\n",
    "metrics = evaluate_model(model, test_x_scaled, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 60713,
     "status": "ok",
     "timestamp": 1729935001978,
     "user": {
      "displayName": "An Nguyen",
      "userId": "10376814865507008633"
     },
     "user_tz": -480
    },
    "id": "8D1jZA3FJXfO",
    "outputId": "0a9443ec-3c38-48de-e40e-2833f4952bcc"
   },
   "outputs": [],
   "source": [
    "#  Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}\")\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    metrics = evaluate_model(model, X_test_tfidf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "executionInfo": {
     "elapsed": 1945630,
     "status": "ok",
     "timestamp": 1729939719976,
     "user": {
      "displayName": "An Nguyen",
      "userId": "10376814865507008633"
     },
     "user_tz": -480
    },
    "id": "ZCmqiepoMngT",
    "outputId": "a43d50c5-21fb-4ddc-b23b-3c605167e088"
   },
   "outputs": [],
   "source": [
    "train_x_scaled = standardize(X_train_tfidf)\n",
    "test_x_scaled = standardize(X_test_tfidf)\n",
    "\n",
    "# SVM = SVC(kernel='rbf', probability=True, class_weight='balanced', random_state=42)\n",
    "# SVM.fit(X_train_tfidf, y_train)\n",
    "# evaluate_model(SVM, X_test_tfidf, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPU0pB5ODCYK/lUAPNnB9Py",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
